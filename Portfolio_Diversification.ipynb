{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a294379f",
   "metadata": {},
   "source": [
    "## InvestOps Tutorial - Portfolio Diversification\n",
    "\n",
    "[Original repository on GitHub](https://github.com/Hvass-Labs/InvestOps-Tutorials)\n",
    "\n",
    "Original author is [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "\n",
    "----\n",
    "\n",
    "From [Monty Python's Life of Brian](https://www.youtube.com/watch?v=KHbzSif78qQ):\n",
    "- Brian: \"*You've got to think for yourselves! You're all individuals!*\"\n",
    "- Crowd in unison: \"*Yes, we're all individuals!*\"\n",
    "- Brian: \"*You're all different!*\"\n",
    "- Crowd in unison: \"*Yes, we're all different!*\"\n",
    "- One man: \"*I'm not!*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07ce74",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial shows how to use the InvestOps functions `diversify_weights` and `diversify_weights_sparse` for improving the diversification of an investment portfolio, by lowering the correlations between different assets in the portfolio. The method is dubbed \"Hvass Diversification\" and is described in Section 8 of the [long paper](#refs) referenced below, and a more concise description is given in the [short paper](#refs). The method is simple, extremely fast to compute, and very robust to estimation errors in the correlation matrix.\n",
    "\n",
    "Briefly explained, given some portfolio weights of how much we want to invest in each asset, the diversification method calculates the so-called \"Full Exposure\" of how much the portfolio is actually exposed to each asset, both through the direct investment in each asset, but also through the correlation with other assets in the portfolio. The diversification method then uses a fairly simple and extremely fast algorithm to find new portfolio weights, whose Full Exposure is equal to the originally desired portfolio weights.\n",
    "\n",
    "Because the diversification method is only allowed to decrease the portfolio weights, the worst that can happen is that it moves too much of the portfolio into cash, which makes it extremely robust to estimation errors in the correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ccb86f",
   "metadata": {},
   "source": [
    "## References <a id=\"refs\"></a>\n",
    "\n",
    "- M.E.H. Pedersen, \"*Simple Portfolio Optimization That Works!*\", 2021. ([PDF](https://ssrn.com/abstract=3942552))\n",
    "\n",
    "- M.E.H. Pedersen, \"*Fast Portfolio Diversification*\", 2022. ([PDF](https://ssrn.com/abstract=4009041))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15efc2",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63722d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Google Colab, automatically install the required\n",
    "# Python packages. This is NOT recommended on your local computer,\n",
    "# unless you have setup a Python environment for this project.\n",
    "# See the README on GitHub for detailed instructions.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install investops numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c18f50",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b181895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages.\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2696e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InvestOps.\n",
    "import investops as iv\n",
    "from investops.diversify import (full_exposure, full_exposure_par,\n",
    "                                 diversify_weights, log_to_dataframe)\n",
    "from investops.random import rand_normal, rand_corr_normal, rand_zero\n",
    "from investops.normalize import normalize_weights\n",
    "from investops.check import fix_corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59487464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InvestOps for sparse correlation matrix.\n",
    "from investops.diversify_sparse import (diversify_weights_sparse,\n",
    "                    full_exposure_sparse, log_to_dataframe_sparse)\n",
    "from investops.sparse import (sparse_corr_to_numpy,\n",
    "                              sparse_to_matrix, matrix_to_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac1ddde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# InvestOps version.\n",
    "iv.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326a713",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d8a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random number generator.\n",
    "# The seed makes the experiments repeatable.\n",
    "rng = np.random.default_rng(seed=80085)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b786b",
   "metadata": {},
   "source": [
    "## Positive Weights &amp; Correlations\n",
    "\n",
    "The first example is very simple. We only have 3 assets in the portfolio and all the portfolio weights and correlations are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0df1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally desired portfolio weights.\n",
    "weights_org = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "# Correlation matrix.\n",
    "corr = np.array([[1.0, 0.5, 0.7],\n",
    "                 [0.5, 1.0, 0.2],\n",
    "                 [0.7, 0.2, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb824516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17233688, 0.21771541, 0.32726136])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and show the Full Exposure of the original weights.\n",
    "full_exposure(weights=weights_org, corr=corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f293a9c",
   "metadata": {},
   "source": [
    "So the portfolio's Full Exposure is actually 17.2% for Asset 1, 21.8% for Asset 2, and 32.7% for Asset 3, even though the original portfolio weights were only 10%, 20%, and 30%. This is because the Full Exposure measures both the portfolio's direct investment in each asset, as well as the indirect exposure through correlations with other assets in the portfolio.\n",
    "\n",
    "The goal is then to find new portfolio weights whose Full Exposure is equal to the originally desired portfolio weights, which is done by the function `diversify_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6587c1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0435443 , 0.18911891, 0.28582913])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and show the adjusted portfolio weights.\n",
    "diversify_weights(weights_org=weights_org, corr=corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616ce07",
   "metadata": {},
   "source": [
    "So we need to invest about 4.3% of the portfolio in Asset 1, 18.9% in Asset 2, and 28.6% in Asset 3, in order for their Full Exposures to equal the originally desired portfolio weights of 10%, 20%, and 30%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b78a2",
   "metadata": {},
   "source": [
    "## Using Asset Names\n",
    "\n",
    "This example shows how to use the diversification function with Pandas Series and DataFrames instead of Numpy arrays, so we can use names for the assets in the portfolio. This example uses the same portfolio weights and correlation matrix from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e12d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock-tickers used as the asset names in the portfolio.\n",
    "names = ['AAPL', 'GOOG', 'MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3951dba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AAPL  GOOG  MSFT\n",
       "AAPL   1.0   0.5   0.7\n",
       "GOOG   0.5   1.0   0.2\n",
       "MSFT   0.7   0.2   1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert correlation matrix from Numpy to Pandas DataFrame.\n",
    "df_corr = pd.DataFrame(data=corr, index=names, columns=names)\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0b03c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "AAPL  0.1\n",
       "GOOG  0.2\n",
       "MSFT  0.3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert original weights from Numpy to Pandas DataFrame.\n",
    "# It is recommended that you use a Pandas Series instead,\n",
    "# but if you need to use a DataFrame, it is important that\n",
    "# it is formatted like this, so it has a single column, the\n",
    "# rows are the original weights for the different assets,\n",
    "# and the index of the rows are the asset names.\n",
    "df_weights_org = pd.DataFrame(data=weights_org, index=names)\n",
    "df_weights_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f917e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL    0.1\n",
       "GOOG    0.2\n",
       "MSFT    0.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert original weights from Numpy to Pandas Series.\n",
    "ser_weights_org = pd.Series(data=weights_org, index=names)\n",
    "ser_weights_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b922dd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL    0.043544\n",
       "GOOG    0.189119\n",
       "MSFT    0.285829\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the diversifier with the weights being a Pandas Series.\n",
    "diversify_weights(weights_org=ser_weights_org, corr=df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a498d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL    0.043544\n",
       "GOOG    0.189119\n",
       "MSFT    0.285829\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the diversifier with the weights being a Pandas DataFrame.\n",
    "diversify_weights(weights_org=df_weights_org, corr=df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a73e58",
   "metadata": {},
   "source": [
    "Note that there is some extra overhead in using Pandas data with the `diversify_weights` function, because it checks the data is correct and also converts it to Numpy for internal processing, and then converts the result to a Pandas Series again.\n",
    "\n",
    "So if you are using the diversification algorithm in a performance-critical application such as back-testing or High-Frequency Trading, then you may want to only pass Numpy arrays to the `diversify_weights` function for faster computation speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b196e3",
   "metadata": {},
   "source": [
    "## Positive / Negative Weights &amp; Correlations\n",
    "\n",
    "This example has both positive and negative portfolio weights and correlations. Some of the correlations are now deemed \"good\" and are therefore not included in the calculation of the Full Exposure, see Section 8.3 in the [paper](#refs) for an explanation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bc33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally desired portfolio weights.\n",
    "weights_org = np.array([0.1, 0.2, -0.3])\n",
    "\n",
    "# Correlation matrix.\n",
    "corr = np.array([[1.0, -0.5, 0.7],\n",
    "                 [-0.5, 1.0, -0.2],\n",
    "                 [0.7, -0.2, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "407e5bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1       ,  0.2059126 , -0.30397368])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and show the Full Exposure of the original weights.\n",
    "full_exposure(weights=weights_org, corr=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2915c1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1       ,  0.1941687 , -0.29613922])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and show the adjusted portfolio weights.\n",
    "diversify_weights(weights_org=weights_org, corr=corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fa66f",
   "metadata": {},
   "source": [
    "So we need to invest 10% of the portfolio in Asset 1 which is the same as the originally desired portfolio weight for Asset 1, because its correlations with the other two assets are deemed \"good\" so they don't require adjustment. But we should only invest 19.4% in Asset 2 and -29.6% in Asset 3. This makes all the Full Exposures equal to the originally desired portfolio weights of 10%, 20%, and -30%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecb26c",
   "metadata": {},
   "source": [
    "## \"Crazy\" Initial Weights\n",
    "\n",
    "This example uses the same portfolio weights and correlations as in the previous example. But now we supply the diversification algorithm with a guess for the adjusted weights which is just \"crazy\" wrong. The diversification algorithm handles this very well, and already after the first iteration it has brought the portfolio weights back into a more normal range. See the sub-section titled \"Crazy Initialization\" in Section 8.12 of the [paper](#refs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d1225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally desired portfolio weights.\n",
    "weights_org = np.array([0.1, 0.2, -0.3])\n",
    "\n",
    "# A \"crazy\" wrong guess for the adjusted portfolio weights.\n",
    "weights_guess = np.array([-123, 0.001, -789])\n",
    "\n",
    "# Correlation matrix.\n",
    "corr = np.array([[1.0, -0.5, 0.7],\n",
    "                 [-0.5, 1.0, -0.2],\n",
    "                 [0.7, -0.2, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "336a6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.50363857e+02,  2.49621714e-01, -8.18580516e+02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and show the Full Exposure of the guessed weights.\n",
    "# These are very far from the originally desired portfolio weights.\n",
    "full_exposure(weights=weights_guess, corr=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c72db44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1       ,  0.1941446 , -0.29615551])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the adjusted portfolio weights, using weights_guess\n",
    "# as a starting guess for the adjusted weights. Log all iterations.\n",
    "log = []\n",
    "diversify_weights(weights_org=weights_org, corr=corr,\n",
    "                  weights_guess=weights_guess, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50c9a1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Full Exp. 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Full Exp. 2</th>\n",
       "      <th>Weight 3</th>\n",
       "      <th>Full Exp. 3</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-123.000000</td>\n",
       "      <td>-250.363857</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.249622</td>\n",
       "      <td>-789.000000</td>\n",
       "      <td>-818.580516</td>\n",
       "      <td>2.441050e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049128</td>\n",
       "      <td>0.049128</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>-0.289159</td>\n",
       "      <td>-0.289175</td>\n",
       "      <td>1.381862e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050905</td>\n",
       "      <td>0.056588</td>\n",
       "      <td>-0.299983</td>\n",
       "      <td>-0.301000</td>\n",
       "      <td>6.856036e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.179916</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>-0.298987</td>\n",
       "      <td>-0.302564</td>\n",
       "      <td>6.940797e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.193667</td>\n",
       "      <td>0.199508</td>\n",
       "      <td>-0.296453</td>\n",
       "      <td>-0.300302</td>\n",
       "      <td>1.111106e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.194145</td>\n",
       "      <td>0.199980</td>\n",
       "      <td>-0.296156</td>\n",
       "      <td>-0.300013</td>\n",
       "      <td>1.918659e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Weight 1  Full Exp. 1  Weight 2  Full Exp. 2    Weight 3  \\\n",
       "Iteration                                                               \n",
       "0         -123.000000  -250.363857  0.001000     0.249622 -789.000000   \n",
       "1            0.049128     0.049128  0.000801     0.003148   -0.289159   \n",
       "2            0.100000     0.100000  0.050905     0.056588   -0.299983   \n",
       "3            0.100000     0.100000  0.179916     0.185800   -0.298987   \n",
       "4            0.100000     0.100000  0.193667     0.199508   -0.296453   \n",
       "5            0.100000     0.100000  0.194145     0.199980   -0.296156   \n",
       "\n",
       "           Full Exp. 3           MSE  \n",
       "Iteration                             \n",
       "0          -818.580516  2.441050e+05  \n",
       "1            -0.289175  1.381862e-02  \n",
       "2            -0.301000  6.856036e-03  \n",
       "3            -0.302564  6.940797e-05  \n",
       "4            -0.300302  1.111106e-07  \n",
       "5            -0.300013  1.918659e-10  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log of all the iterations of weight adjustments.\n",
    "log_to_dataframe(weights_org=weights_org, corr=corr, log=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ce012",
   "metadata": {},
   "source": [
    "The diversification algorithm corrected the \"crazy\" wrong guess for the portfolio weights already after the first iteration. The bottom-row of the log shows the same adjusted portfolio weights as in the previous example, which was initialized with the original portfolio weights.\n",
    "\n",
    "Also note how the Mean Squared Error (MSE) between the Full Exposure and the originally desired weights decrease by several orders of magnitude in most iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dba09b",
   "metadata": {},
   "source": [
    "## Higher Precision\n",
    "\n",
    "This example uses the same portfolio weights and correlations as in the previous example, but it uses the default weight-initialization instead of the \"crazy\" wrong guess. And now we want an even higher precision in the adjusted portfolio weights, so their Full Exposures are even closer to the originally desired weights. This is done by setting the argument `tol` (short for \"error **tol**erance\") to a value that is close to, but not equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05da0947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1       ,  0.19416484, -0.29614184])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the adjusted portfolio weights with higher precision.\n",
    "# Log all iterations.\n",
    "log = []\n",
    "diversify_weights(weights_org=weights_org, corr=corr,\n",
    "                  tol=1e-20, log=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "899f7d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Full Exp. 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Full Exp. 2</th>\n",
       "      <th>Weight 3</th>\n",
       "      <th>Full Exp. 3</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.205913</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.303974</td>\n",
       "      <td>1.691634e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194257</td>\n",
       "      <td>0.200091</td>\n",
       "      <td>-0.296078</td>\n",
       "      <td>-0.299938</td>\n",
       "      <td>4.039732e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194169</td>\n",
       "      <td>0.200004</td>\n",
       "      <td>-0.296139</td>\n",
       "      <td>-0.299997</td>\n",
       "      <td>6.963000e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>1.200798e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>2.070865e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>3.571366e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>6.159123e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>1.061951e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>1.841171e-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>2.845241e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>1.027163e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.296142</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weight 1  Full Exp. 1  Weight 2  Full Exp. 2  Weight 3  \\\n",
       "Iteration                                                           \n",
       "0               0.1          0.1  0.200000     0.205913 -0.300000   \n",
       "1               0.1          0.1  0.194257     0.200091 -0.296078   \n",
       "2               0.1          0.1  0.194169     0.200004 -0.296139   \n",
       "3               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "4               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "5               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "6               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "7               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "8               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "9               0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "10              0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "11              0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "12              0.1          0.1  0.194165     0.200000 -0.296142   \n",
       "\n",
       "           Full Exp. 3           MSE  \n",
       "Iteration                             \n",
       "0            -0.303974  1.691634e-05  \n",
       "1            -0.299938  4.039732e-09  \n",
       "2            -0.299997  6.963000e-12  \n",
       "3            -0.300000  1.200798e-14  \n",
       "4            -0.300000  2.070865e-17  \n",
       "5            -0.300000  3.571366e-20  \n",
       "6            -0.300000  6.159123e-23  \n",
       "7            -0.300000  1.061951e-25  \n",
       "8            -0.300000  1.841171e-28  \n",
       "9            -0.300000  2.845241e-31  \n",
       "10           -0.300000  1.027163e-33  \n",
       "11           -0.300000  0.000000e+00  \n",
       "12           -0.300000  0.000000e+00  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log of all the iterations of weight adjustments.\n",
    "log_to_dataframe(weights_org=weights_org, corr=corr, log=log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5948",
   "metadata": {},
   "source": [
    "Compared to the previous section that used the default argument `tol=1e-3` and which needed 5 iterations of the diversification algorithm to converge, we now need 11 iterations of the algorithm when `tol=1e-20`.\n",
    "\n",
    "Note that the Mean Squared Error (MSE) decreases by several orders of magnitude in each iteration and eventually becomes zero, which means the Full Exposures have become exactly equal to the originally desired portfolio weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1bf9b",
   "metadata": {},
   "source": [
    "## Time Usage\n",
    "\n",
    "Let us now measure how long it takes to use the diversification algorithm on a portfolio with 1000 assets. We will generate random portfolio weights and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1e3ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of assets in the portfolio.\n",
    "num_assets = 1000\n",
    "\n",
    "# Originally desired portfolio weights.\n",
    "# These are randomly generated from a normal distribution.\n",
    "weights_org = rand_normal(rng=rng, size=num_assets,\n",
    "                          mean=0.0, std=0.05, low=-0.2, high=0.2)\n",
    "\n",
    "# Correlation matrix randomly generated from a normal distribution.\n",
    "corr = rand_corr_normal(rng=rng, num_assets=num_assets, mean=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a26887e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the randomly generated portfolio weights.\n",
    "plt.hist(weights_org, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c42182a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the randomly generated correlation coefficients.\n",
    "plt.hist(corr.flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1006ade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 ms  1.02 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Measure the time-usage of the diversification algorithm.\n",
    "diversify_weights(weights_org=weights_org, corr=corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ebc49",
   "metadata": {},
   "source": [
    "So the diversification algorithm only needed about **20 milli-seconds** to converge to a near-optimal solution for a portfolio with 1000 assets. This was run on a computer with a 2.6 GHz CPU (3.5 GHz boost). The time-usage obviously depends on the number of assets in the portfolio, the weights and correlations, and the speed of the computer.\n",
    "\n",
    "The time-complexity is quadratic if you hold the error tolerance fixed (which is the `tol` argument in the `diversify_weights` function). This means that for a portfolio of 10,000 assets, you should expect a time-usage of 10\\*10\\*20 milli-seconds = 2 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ad4ab",
   "metadata": {},
   "source": [
    "## Compare Weights Before &amp; After\n",
    "\n",
    "Let us now compare the portfolio weights before and after they were adjusted by the diversification algorithm. This uses the same portfolio weights and correlations that were randomly generated in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fccdaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the adjusted portfolio weights.\n",
    "weights_new = diversify_weights(weights_org=weights_org, corr=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d8c264f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeRElEQVR4nO3dfZRcdZ3n8fenO5XYYffQCUSEDpkEiXhgUdBacJaZXUYegs5iMggbfNjN2WE3sw+cPaNuxuTASkQcwkRF5+hZTxbxgKIEGaZtB8ZeJDC7hxEmDR1s4xITUCAFSiQJDqQhneS7f9Ttprqo6nR33Xrq+3md06fr3vurqt/th/ut+/v+HhQRmJlZdnU0uwJmZtZcDgRmZhnnQGBmlnEOBGZmGedAYGaWcbOaXYHpOP7442Px4sXNroaZWVt57LHHfhMRC8r3pxIIJF0CfAXoBG6JiA1lx+cAtwPvBV4CVkbELyXlgFuA9yR1uT0ibjza+y1evJiBgYE0qm5mlhmSnqm0v+amIUmdwNeADwCnAx+RdHpZsauAfRFxKnAzcFOy/wpgTkScSTFI/ImkxbXWyczMJi+NHME5wK6IeDoiDgJ3AsvLyiwHbkse3w1cIElAAMdImgV0AQeB36ZQJzMzm6Q0AkEP8FzJ9u5kX8UyEXEIeBk4jmJQeBV4AXgW+EJE7K30JpJWSxqQNLBnz54Uqm1mZtD8XkPnAIeBk4AlwKcknVKpYERsioh8ROQXLHhTrsPMzKYpjUBQAE4u2V6Y7KtYJmkGOpZi0vijwA8jYiQiXgQeBvIp1MnMzCYpjUCwFVgqaYmk2cCVQF9ZmT5gVfL4cmBLFGe7exZ4P4CkY4D3AU+mUCczsxmjd7DAeRu2sGTtvZy3YQu9g+WftWtTc/fRiDgk6Wqgn2L30VsjYruk64GBiOgDvgF8S9IuYC/FYAHF3kbflLQdEPDNiPhJrXUyM5spegcLrLtniOGRwwAU9g+z7p4hAFacXZ6OnR614zTU+Xw+PI7AzLLgvA1bKOwfftP+nu4uHl77/im9lqTHIuJNze/NThabmdkEnq8QBCbaPx0OBGZmLeyk7q4p7Z8OBwIzsxa2ZtlpdOU6x+3rynWyZtlpqb1HW046Z2aWFaMJ4Y39O3h+/zAndXexZtlpqSWKwYHAzKzlrTi7J9ULfzk3DZmZZZwDgZlZxrlpyMysBfQOFuqaB5iIA4GZWZM1YvTwRNw0ZGbWZBv7d4wFgVHDI4fZ2L+jIe/vQGBm1mSNGD08EQcCM7Mma8To4Yk4EJiZNVkjRg9PxMliM7Mma8To4Yk4EJiZtYB6jx6eiJuGzMwyzoHAzCzjHAjMzDLOgcDMLOMcCMzMMs6BwMws41IJBJIukbRD0i5JayscnyNpc3L8UUmLS469S9KPJW2XNCTpLWnUyczMJqfmQCCpE/ga8AHgdOAjkk4vK3YVsC8iTgVuBm5KnjsL+DbwnyLiDOB8YKTWOpmZ2eSlcUdwDrArIp6OiIPAncDysjLLgduSx3cDF0gScDHwk4h4AiAiXoqIw5iZWcOkEQh6gOdKtncn+yqWiYhDwMvAccA7gJDUL+lxSX9W7U0krZY0IGlgz549KVTbzMyg+cniWcDvAR9Lvv+RpAsqFYyITRGRj4j8ggULGllHM7MZLY1AUABOLtlemOyrWCbJCxwLvETx7uH/RMRvIuIAcB/wnhTqZGZmk5RGINgKLJW0RNJs4Eqgr6xMH7AqeXw5sCUiAugHzpQ0NwkQ/wr4WQp1MjOzSap59tGIOCTpaooX9U7g1ojYLul6YCAi+oBvAN+StAvYSzFYEBH7JH2JYjAJ4L6IuLfWOpmZ2eSp+MG8veTz+RgYGGh2NczM2oqkxyIiX76/2cliMzNrMgcCM7OMcyAwM8s4BwIzs4xzIDAzyzgHAjOzjHMgMDPLOAcCM7OMcyAwM8s4BwIzs4xzIDAzyzgHAjOzjKt59lEzs5mkd7DAxv4dPL9/mJO6u1iz7DRWnF2+6OLM4kBgZplVftH/g3cu4K8eKzA8Ulw6vbB/mE9s3sbAM3u5YcWZTa5t/bhpyMwyqXewwLp7hijsHyYoXvTveOTZsSAwKoA7HnmW3sHyhRdnDgcCM8ukjf07Kl70K4mk/EzlQGBmmfT8/uEplS9MsXw7cY7AzDKpe26OfQdGpvScsz77v3l5eGTGJZEdCMwsM0qTw9Oxf7gYOAr7h1l3zxDAjAgGbhoys0woTw7Xulr78MjhGZM3cCAws0yolByu1XTvLFpNKoFA0iWSdkjaJWltheNzJG1Ojj8qaXHZ8UWSXpH039Ooj5lZuXpctE/q7kr9NZuh5kAgqRP4GvAB4HTgI5JOLyt2FbAvIk4FbgZuKjv+JeBva62LmVk11S7anRIC5s3NkevQuGOjW5WOdeU6WbPstDrUtPHSSBafA+yKiKcBJN0JLAd+VlJmObA+eXw38FVJioiQtAL4BfBqCnUxM6tozbLTWHfP0Ljmoa5cJzdeduZYwnei6SVm8tQTaQSCHuC5ku3dwLnVykTEIUkvA8dJeg34NHARMGGzkKTVwGqARYsWpVBtM8uS0Yv2RBfzFWf3VL24T3Ss3TW7++h64OaIeEXShAUjYhOwCSCfz9ea8DezDCn/NH/zyrNm7EV9OtIIBAXg5JLthcm+SmV2S5oFHAu8RPHO4XJJfwF0A0ckvRYRX02hXmZmY91GSyeSW/O9J/jsD7az/8DMGxw2HWkEgq3AUklLKF7wrwQ+WlamD1gF/Bi4HNgSEQH8/mgBSeuBVxwEzGy6KrXjV+o2OnIkxkYVz7TBYdNRc6+hiDgEXA30A/8PuCsitku6XtKHkmLfoJgT2AV8EnhTF1Mzs1pUmk10dPtoZtLgsOlIJUcQEfcB95Xt+0zJ49eAK47yGuvTqIuZZcvoXUClC/7wyGEkiElkFWfK4LDpaHay2MxsSia68FcymSAAM2dw2HQ4EJhZ2yhP/KZlJg0Omw7PNWRmbSPN+YJ6urtQ8r10UFkW+Y7AzNpGWu343V05Hl77/lReaybwHYGZtY1ju3KpvM6rBw/N6DWIp8qBwMzaxsjhIym9TmS6u2g5Nw2ZWcu6tneI7z76HIcjELUvJlMqy91FyzkQmFlLurZ3iG8/8uzYdtoTjGW5u2g5Nw2ZWUv67qPPHb1Qia5cJx9/3yK6cp2TKpvl7qLlHAjMrCUdnuxIsMSH39vDDSvO5MbLzhzXNfTLK8/iyyvPcnfRCbhpyMyaotpCL72DBT77g+1Tfr0Hn9wDVF83wBf+6hwIzKzhKk0Nve6eIQae2cvmrc8xcnjqGQEnf6fPgcDMGq7SCOHhkcNjPYSmw8nf6XMgMLO6qtQEVO3T+3SDQK5TTv7WwIHAzOqmWhNQ99zc2MIw01E6pmDe3BzXXXqGcwA1cCAws7qp1gQ0Z1YHuU5NKxeQ6xAbr3i3L/wpciAws7qp1gS0f3h6dwMSDgJ14HEEZlY3qSdww91A68F3BGaWutI5gtLknkH14UBgZqnpHSxwzV8P8erBdFcQA08LUU8OBGaWinosIzm68HxPychjS18qgUDSJcBXgE7glojYUHZ8DnA78F7gJWBlRPxS0kXABmA2cBBYExFb0qiTmTVWGstIzpubY/+BkXFTTlj91RwIJHUCXwMuAnYDWyX1RcTPSopdBeyLiFMlXQncBKwEfgNcGhHPS/pnQD/g37xZm+kdLFBIYYqH10aOcPPKsxwAGiyNO4JzgF0R8TSApDuB5UBpIFgOrE8e3w18VZIiYrCkzHagS9KciHg9hXqZWR2UjxRefFwXf//U3lRee3jkMBv7dzgQNFgagaAHKJ04fDdwbrUyEXFI0svAcRTvCEZ9GHi8WhCQtBpYDbBo0aIUqm1mU1VppHAadwKlPHlc47XEOAJJZ1BsLvqTamUiYlNE5CMiv2DBgsZVzszGrO/bnmoyuBJ3EW28NAJBATi5ZHthsq9iGUmzgGMpJo2RtBD4a+DfRcRTKdTHzOqgd7Aw7RHBk+Uuos2RRiDYCiyVtETSbOBKoK+sTB+wKnl8ObAlIkJSN3AvsDYiHk6hLmZWJxv7d6T6ep3SuO9eOax5as4RJG3+V1Ps8dMJ3BoR2yVdDwxERB/wDeBbknYBeykGC4CrgVOBz0j6TLLv4oh4sdZ6mVm60swFCHjqxg+m9npWm1TGEUTEfcB9Zfs+U/L4NeCKCs+7AbghjTqYWX11SqlNGeE8QGvxyGIze5NKi8lMJwh0dogOYOTIG891HqD1OBCY2bhJ4kRxaofRa3dh/zB/unnblF+zU+KLV7wboOIi9dY6HAjMMu7a3iG+/cizY9tBcX6fWgj44r95Y90AX/hbmwOBWUaNNv+kPSAM4GPvW+SLfxtxIDDLoN7BAp/cvI0jdXr9G1acWadXtnpoiZHFZtZY9QwCPe4R1HYcCMwypHewwClr761bEHCPoPbkpiGzGayeeQAorh8QAS8Pew2BduZAYDZD1WPFsFI93V08vPb9dXltaywHArM2Vz746w/euYAHn9xTt7sAcBPQTONAYNbGKq0PUDomIE2dEkci3AQ0AzkQmLWxNNYJnoyuXKdnBp3BHAjM2lDplBD10N2V45g5szwtREY4EJi1kY/9rx/zcErrA1fTletk/YfO8IU/QxwIzFpQabdPUZz/p17cBdQcCMxaTKVJ4Orh4+9b5KkgDHAgMGu63sEC6/u213094FFO/Fo5BwKzJukdLPCJzdvq2uwzam6ug+GRI276sYocCMwa5F3X/ZDfvl7/rp6l5uY6+PPL3uULv03IgcCsDhrRu2ciX155li/+NmkOBGZT1DtYYM33tjFSryk8p+mY2Z18/o/c9m9Tl0ogkHQJ8BWgE7glIjaUHZ8D3A68F3gJWBkRv0yOrQOuAg4D/y0i+tOok1mpcz9/P7/+x4PNrkbqetzmbymoORBI6gS+BlwE7Aa2SuqLiJ+VFLsK2BcRp0q6ErgJWCnpdOBK4AzgJOBHkt4REY1tSLW2V97lcqYSOOFrqUvjjuAcYFdEPA0g6U5gOVAaCJYD65PHdwNflaRk/50R8TrwC0m7ktf7cQr1shmod7DAp+7axuFGdLVpAU72WiOkEQh6gOdKtncD51YrExGHJL0MHJfsf6TsuRX/4iWtBlYDLFq0KIVqW7uYqc06lbipx5qhbZLFEbEJ2ASQz+cz8nkwe5rd26bRch2w8Qr38LHmSiMQFICTS7YXJvsqldktaRZwLMWk8WSeazNYvZdSbCWe0sFaVRqBYCuwVNISihfxK4GPlpXpA1ZRbPu/HNgSESGpD/iOpC9RTBYvBf4hhTpZi5tpn/yXvvUY7v/k+c2uhtm01BwIkjb/q4F+it1Hb42I7ZKuBwYiog/4BvCtJBm8l2KwICl3F8XE8iHgv7rH0MzWyGkVqnlLp3jy8x9sYg3MWouiTgtb1FM+n4+BgYFmV8OOonewwJ/d/QQHm9TFx4lXs/EkPRYR+fL9bZMstvbQO1jgsz/Yzr4DjZlJc5RH1ZpNnwOBpaZ3sMAn79rGkQbdAPgTv1k6HAisZtf2DvGdR5+tewDoynVwowdXmaXOgcBq0ojeP/Pm5rjuUq+ha1YvDgQ2LfXMBXR35bx4ulkDORBYVaWDvTolDtexh9mcWR3c9GE3+5g1gwOBVVQ+m2e9goCbfcyaz4HA3qR3sFD3KZ09q6ZZ63AgsHFGu4DWgz/9m7UmBwIbU1yC8YlUu4F6ojWz1udAkDGjCeDn9w+PrXQFpD4DqPv8m7UPB4IM6R0ssO6eIYZHivP6FfYP86ebt6X6Hp7qwaz9OBBkyMb+HWNBIE2egtmsvXU0uwLWOM/XYfGX894+30HArM35jiAjru0dSnUNgFyH2HjFu90EZDYD+I4gA8oHh9WquyvnIGA2g/iOYAYq7RnUlevgwMiRml/TYwDMZi4Hghnk2t4h7njk2XFNQLUGgU7BUzf+YW0VM7OW5kDQ5konhquHRi0yY2bN40DQxsrHBdTDSd1ddXttM2sNTha3sXqNCxjVlescG3lsZjNXTYFA0nxJ90vamXyfV6XcqqTMTkmrkn1zJd0r6UlJ2yVtqKUuWZTWuAAlX/Pm5ujuyiGK6wHfeJlHCJtlQa1NQ2uBByJig6S1yfanSwtImg9cB+SBAB6T1Ae8DnwhIh6UNBt4QNIHIuJva6zTjFU+T9Dc2Z28ejCdO4JfbHBC2Cyram0aWg7cljy+DVhRocwy4P6I2BsR+4D7gUsi4kBEPAgQEQeBx4GFNdZnxhrNBxT2DxMU5wlKKwg4D2CWbbUGghMi4oXk8a+AEyqU6QGeK9nenewbI6kbuBR4oNobSVotaUDSwJ49e2qqdDuqVz4g1ynnAcwy7qhNQ5J+BLytwqFrSjciIiRNubOhpFnAd4G/jIinq5WLiE3AJoB8Pp+5To31mCfIg8TMDCYRCCLiwmrHJP1a0okR8YKkE4EXKxQrAOeXbC8EHirZ3gTsjIgvT6bCWXVsV479wyM1vUZ3V471H/KF38zGqzVZ3AesAjYk379foUw/8OclPYouBtYBSLoBOBb4DzXWY8YpTQx3z609CHilMDOrptYcwQbgIkk7gQuTbSTlJd0CEBF7gc8BW5Ov6yNir6SFFJuXTgcel7RNkgMCb04M7zsw/SAgHATMbGI13RFExEvABRX2D1DyKT8ibgVuLSuzm+J1ysrUmhjuUHFqiJ5kKUo3BZnZRDzFRAua7rxBvvCb2XR4iokWc23v0LSfW9g/zLp7hugdLKRYIzOb6RwIWkjvYIE7alxAZnjkMBv7d6RUIzPLAgeCFrKxf0cqy0nWY8yBmc1czhE0UO9ggfV928e6gh4zu5NcZwcvD49wUndXamsKeMoIM5sKB4IG6R0ssOZ7TzBSstJLca6gYu+g6QSBrlwHoHE9jDx1tJlNlQNBg2zs3zEuCExVrkPjnp/rEDde9q6x1x6dkdS9hsxsqhwIGqTWdvuNV7y76gXfF34zq4UDQYPUmgNYcXaPL/hmVhfuNdQga5adRq5jegOpz3v7/JRrY2b2BgeCBllxdg8br3g33V25KT3vvLfP547/+Lt1qpWZmZuG6q58ecnRaaAr9SIq1ZXr9JrBZtYQDgR1dG3vEHc88uzYILHRKSAGntnLg0/uYeRIIBg7LkF4sjgzazAHgpSN3gFUSwwPjxweFxwCf/o3s+ZyjiBFpesITKS8McjzA5lZMzkQpKiWdQQ8P5CZNYsDQYpquZh7fiAzaxYHghRN5mI+Z1bHm8YTeH4gM2smB4IUrVl2Gl25zgnLvH7oCCNHgq5cB6LYQ8iJYjNrJvcaStHoxbx03MCBg4cqLj7/2sgRbl55lgOAmTWdA0HKyucEWrL23orlgmLAcCAws2arqWlI0nxJ90vamXyfV6XcqqTMTkmrKhzvk/TTWurSqibKG7inkJm1glpzBGuBByJiKfBAsj2OpPnAdcC5wDnAdaUBQ9JlwCs11iMVvYMFztuwhSVr7+W8DVvGLQI/0bGJrFl2GtWmmnNPITNrBbU2DS0Hzk8e3wY8BHy6rMwy4P6I2Asg6X7gEuC7kv4J8ElgNXBXjXWpyehgsNFxAKPTQYyqduxoTTsrzu5h4Jm940YTg3sKmVnrqPWO4ISIeCF5/CvghApleoDnSrZ3J/sAPgd8EThwtDeStFrSgKSBPXv21FDlyioNBhsd8TvRscm4YcWZ3LzyLHq6u9xTyMxazlHvCCT9CHhbhUPXlG5EREia9FqMks4C3h4Rn5C0+GjlI2ITsAkgn89Pf83HKqq110/Ujj+VNn4vLGNmreqogSAiLqx2TNKvJZ0YES9IOhF4sUKxAm80HwEspNiE9LtAXtIvk3q8VdJDEXE+TVBtBbEOicNROe5IxSYlX+DNrJ3V2jTUB4z2AloFfL9CmX7gYknzkiTxxUB/RPzPiDgpIhYDvwf8vFlBAKoPBqsWBACOBKy5+4lJJ47NzFpRrYFgA3CRpJ3Ahck2kvKSbgFIksSfA7YmX9ePJo5byYqze7jxsjPH2vE7NbllJUcOh2cONbO2ppjgE2+ryufzMTAwUNf3WLL23jdNF12NgF9s+MN6VsfMrGaSHouIfPn+TI4sLl8+stJqYNVyBpV0z53aOsRmZq0kc5POXds7xCc2b6Owf5jgjTEB5e38k5lAbtQrrx1ynsDM2lamAkHvYOFNA7ug8piA8pxBT3cXH3/fooqjhEeOOE9gZu0rU01DG/t3VG33rzQmoFLf/zseeXbSzzczaweZCgQTXay75+Y4b8OWCfMGUD134HmDzKxdZappaKKL9SuvHTpq3gAq5w48b5CZtbNMBYJKF3EBXbkORo6MbzSqNpdQpdyB5w0ys3aWqaahSiuIrVl2Gp/YvK1i+WpNSZ43yMxmkkwFAqh8Ed/Yv8Pt/maWWZlqGqrG7f5mlmWZuyOopFqTkZt/zCwLHAgSbvc3s6xy05CZWcY5EJiZZZwDgZlZxmUmRzCZqafNzLIoE4Ggd7DAunuGGB45DLwxhQTgYGBmmZeJpqGN/TvGgsCoalNImJllTSYCQbWpIjx1tJlZRgJBtakiPIWEmVmNgUDSfEn3S9qZfJ9XpdyqpMxOSatK9s+WtEnSzyU9KenDtdSnGk8hYWZWXa13BGuBByJiKfBAsj2OpPnAdcC5wDnAdSUB4xrgxYh4B3A68Hc11qciTx1tZlZdrb2GlgPnJ49vAx4CPl1WZhlwf0TsBZB0P3AJ8F3gj4F3AkTEEeA3NdanKk8hYWZWWa13BCdExAvJ418BJ1Qo0wM8V7K9G+iR1J1sf07S45K+J6nS883MrI6OGggk/UjSTyt8LS8tFxEBVdeGr2QWsBD4+4h4D/Bj4AsT1GO1pAFJA3v27JnC25iZ2USO2jQUERdWOybp15JOjIgXJJ0IvFihWIE3mo+gePF/CHgJOADck+z/HnDVBPXYBGwCyOfzUwk4ZmY2gVqbhvqA0V5Aq4DvVyjTD1wsaV6SJL4Y6E/uIH7AG0HiAuBnNdbHzMymqNZAsAG4SNJO4MJkG0l5SbcAJEnizwFbk6/rRxPHFBPL6yX9BPi3wKdqrI+ZmU2Rih/M24ukPcAzza5HCo6njj2lmsDn09p8Pq2tEefzOxGxoHxnWwaCmULSQETkm12PtPh8WpvPp7U183wyMcWEmZlV50BgZpZxDgTNtanZFUiZz6e1+XxaW9POxzkCM7OM8x2BmVnGORCYmWWcA0GdTWHNhh9K2i/pb8r2L5H0qKRdkjZLmt2YmleWwhoUD0naIWlb8vXWxtV+XP0uSeqxS1Kl6dPnJD/vXcnPf3HJsXXJ/h2SljW04lVM93wkLZY0XPL7+HrDK1/BJM7nXyaTVR6SdHnZsYp/e81U4/kcLvn99NWlghHhrzp+AX8BrE0erwVuqlLuAuBS4G/K9t8FXJk8/jrwn1v9fID5wNPJ93nJ43nJsYeAfJPPoRN4CjgFmA08AZxeVua/AF9PHl8JbE4en56UnwMsSV6ns43PZzHw02bWf5rnsxh4F3A7cPlk/vba8XySY6/Uu46+I6i/5RTXaiD5vqJSoYh4APjH0n2SBLwfuPtoz2+gyZzP2BoUEbEPGF2DolWcA+yKiKcj4iBwJ8XzKlV6nncDFyS/j+XAnRHxekT8AtiVvF4z1XI+reio5xMRv4yInwBHyp7bin97tZxPQzgQ1N9k1myo5jhgf0QcSrZ3U1zfoZmmvQZFyfY3k9vc/9Gki9HR6jeuTPLzf5ni72Myz220Ws4HYImkQUl/J+n3613ZSajlZ9yuv5+JvCWZgv8RSStSrVmi1hXKjOKaDcDbKhy6pnQjIkJSy/fXrfP5fCwiCpL+KfBXFCcbvH16NbUUvAAsioiXJL0X6JV0RkT8ttkVszG/k/zPnAJskTQUEU+l+QYOBCmI2tdsqOYloFvSrORT3EKK6zvUVQrnU20NCiKikHz/R0nfoXjb3OhAUABOLqtf+c91tMxuSbOAYyn+Pibz3Eab9vlEsRH6dYCIeEzSU8A7gIG617q6Wn7GVf/2mqimv5mS/5mnJT0EnE0x55AaNw3V32TWbKgo+Sd9EBjtRTCl59fJtNegkDRL0vEAknLAvwZ+2oA6l9sKLE16ZM2mmDwt741Rep6XA1uS30cfcGXSC2cJsBT4hwbVu5ppn4+kBZI6AZJPnEspJlibaTLnU03Fv7061XOypn0+yXnMSR4fD5xHPdZtaWY2PQtfFNthHwB2Aj8C5if788AtJeX+L7AHGKbYhrgs2X8KxQvNLoqruM1pk/P546TOu4B/n+w7BngM+AmwHfgKTepxA3wQ+DnFT1bXJPuuBz6UPH5L8vPelfz8Tyl57jXJ83YAH2j231gt5wN8OPldbAMeBy5t9rlM8nz+efJ/8irFO7XtE/3tNftruucD/AtgiGJPoyHgqnrUz1NMmJllnJuGzMwyzoHAzCzjHAjMzDLOgcDMLOMcCMzMMs6BwMws4xwIzMwy7v8DAIVdWt3CUjoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter-plot of the original portfolio weights on the x-axis\n",
    "# and the adjusted portfolio weights on the y-axis.\n",
    "plt.scatter(weights_org, weights_new);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6403c",
   "metadata": {},
   "source": [
    "It appears that the diversification algorithm \"compresses\" the portfolio weights in a smooth but also non-linear way, so the weights that were originally closer to zero get moved dis-proportionally closer to zero. This of course depends on the exact values of the portfolio weights and correlations. It would be an interesting topic for you to research further. Perhaps it could lead to deeper insights into why the diversification algorithm works so well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8d3aa",
   "metadata": {},
   "source": [
    "## Weight Normalization\n",
    "\n",
    "The portfolio weights that are output from the function `diversify_weights` are not normalized, so they can sum to more than 1. This example shows how to normalize the portfolio weights using the function `normalize_weights` which can handle both positive and negative weights. We are reusing the weights and correlations from the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2339e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the portfolio weights so the positive weights\n",
    "# sum to max 1.0, the negative weights sum to min -0.5,\n",
    "# and the ratio between the sums of positive and negative\n",
    "# weights is at most 0.2.\n",
    "weights_norm, cash = \\\n",
    "    normalize_weights(weights=weights_new, limit_pos=1.0,\n",
    "                      limit_neg=-0.5, max_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4347811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the weights before (x-axis) and after (y-axis) normalization.\n",
    "plt.scatter(weights_new, weights_norm);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afdfa7c",
   "metadata": {},
   "source": [
    "Note that the function `normalize_weights` also accepts input-weights as 2-dimensional Numpy Arrays or Pandas DataFrames, where the rows are for time-steps and the columns are for the different assets. So you can use the function to normalize the weights for many time-steps at once, e.g. for use in back-testing an investing strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e501a3de",
   "metadata": {},
   "source": [
    "## Sparse Correlation Matrix\n",
    "\n",
    "We will now use a so-called sparse correlation matrix whose elements are mostly zero. We can ignore correlations that are zero because they will not have any effect on the calculation of the Full Exposure. This can speed up the diversification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e42ba",
   "metadata": {},
   "source": [
    "### Sparse Correlation - Small Example\n",
    "\n",
    "We begin with an example of a small portfolio with only 3 stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c51c312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of assets in the portfolio.\n",
    "num_assets = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52e66916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSFT    0.1\n",
       "BBBY    0.3\n",
       "AAPL    0.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Originally desired portfolio weights.\n",
    "weights_org = pd.Series(dict(MSFT=0.1, BBBY=0.3, AAPL=0.2))\n",
    "weights_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e29af41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse correlation matrix.\n",
    "# Here the correlation between stocks MSFT and BBBY is 0.5\n",
    "# and the correlation between stocks AAPL and BBBY is 0.1\n",
    "corr_sparse = [('MSFT', 'BBBY', 0.5), ('AAPL', 'BBBY', 0.1)]\n",
    "\n",
    "# WARNING! The diagonal and symmetrical correlations are omitted,\n",
    "# because the diversification algorithm handles this automatically.\n",
    "# If you include the diagonal and lower-triangle matrix then the\n",
    "# diversification algorithm gives the wrong result! So you should\n",
    "# only use the upper triangle of the correlation matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cd8e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_i: ('MSFT', 'AAPL')\n",
      "corr_j: ('BBBY', 'BBBY')\n",
      "corr_coef: (0.5, 0.1)\n"
     ]
    }
   ],
   "source": [
    "# The above format for a sparse correlation matrix is easy to\n",
    "# write and understand, but cannot be used directly by the Python\n",
    "# functions for diversifying the portfolio, because they use the\n",
    "# Numba Jit compiler to speed up the runtime, and that currently\n",
    "# does not support the above format properly so it runs slowly.\n",
    "# So we need to convert the sparse matrix to another format.\n",
    "\n",
    "# Convert the sparse correlation matrix into 3 tuples.\n",
    "corr_i, corr_j, corr_coef = zip(*corr_sparse)\n",
    "\n",
    "# Print the 3 tuples that define the sparse correlation matrix.\n",
    "print(f'corr_i: {corr_i}')\n",
    "print(f'corr_j: {corr_j}')\n",
    "print(f'corr_coef: {corr_coef}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2b8e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_org_np: [0.1 0.3 0.2]\n",
      "corr_i_np: [0 2]\n",
      "corr_j_np: [1 1]\n",
      "corr_coef_np: [0.5 0.1]\n"
     ]
    }
   ],
   "source": [
    "# Convert the weights and sparse correlation matrix into Numpy\n",
    "# arrays which use integers instead of stock-tickers to identify\n",
    "# the assets in the portfolio.\n",
    "weights_org_np, corr_i_np, corr_j_np, corr_coef_np = \\\n",
    "    sparse_corr_to_numpy(weights=weights_org, corr_i=corr_i,\n",
    "                         corr_j=corr_j, corr_coef=corr_coef)\n",
    "\n",
    "# Print the weights and Numpy arrays that define the sparse\n",
    "# correlation matrix. This is in so-called Coordinate (COO) format.\n",
    "print(f'weights_org_np: {weights_org_np}')\n",
    "print(f'corr_i_np: {corr_i_np}')\n",
    "print(f'corr_j_np: {corr_j_np}')\n",
    "print(f'corr_coef_np: {corr_coef_np}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "392abcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.5, 0. ],\n",
       "       [0.5, 1. , 0.1],\n",
       "       [0. , 0.1, 1. ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dense correlation matrix, so we can compare the sparse\n",
    "# and dense versions of the diversification algorithm later.\n",
    "corr_dense = sparse_to_matrix(sparse_i=corr_i_np, sparse_j=corr_j_np, sparse_v=corr_coef_np,\n",
    "                              shape=(num_assets, num_assets))\n",
    "\n",
    "# Make the correlation matrix symmetrical.\n",
    "corr_dense += corr_dense.T\n",
    "\n",
    "# Fill the diagonal with 1.\n",
    "np.fill_diagonal(corr_dense, 1.0)\n",
    "\n",
    "# Show the dense correlation matrix.\n",
    "corr_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "323bf084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13228757, 0.3132092 , 0.20149442])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Full Exposure using the SPARSE correlation matrix.\n",
    "# NOTE: This only works with Numpy arrays! You cannot use Pandas\n",
    "# data, and you should not use Python lists because it is slow!\n",
    "full_exposure_sparse(weights=weights_org_np, corr_i=corr_i_np,\n",
    "                     corr_j=corr_j_np, corr_coef=corr_coef_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2384361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13228757, 0.3132092 , 0.20149442])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Full Exposure using the DENSE correlation matrix.\n",
    "full_exposure(weights=weights_org_np, corr=corr_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "467e1ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07020497, 0.29034873, 0.19855427])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diversify the portfolio weights using the SPARSE correlation matrix.\n",
    "# This uses Numpy arrays as input.\n",
    "diversify_weights_sparse(weights_org=weights_org_np,\n",
    "                         corr_i=corr_i_np, corr_j=corr_j_np,\n",
    "                         corr_coef=corr_coef_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c429dcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07020497, 0.29034873, 0.19855427])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diversify the portfolio weights using the DENSE correlation matrix.\n",
    "# This uses Numpy arrays as input.\n",
    "diversify_weights(weights_org=weights_org_np, corr=corr_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4da59a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSFT    0.070205\n",
       "BBBY    0.290349\n",
       "AAPL    0.198554\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diversify the portfolio weights using the SPARSE correlation matrix.\n",
    "# This uses Pandas Series for the weights and lists of strings for\n",
    "# corr_i and j, which are automatically converted to Numpy arrays.\n",
    "# The iterations of the algorithm are also recorded in the log.\n",
    "log_sparse = []\n",
    "diversify_weights_sparse(weights_org=weights_org, corr_i=corr_i,\n",
    "                         corr_j=corr_j, corr_coef=corr_coef,\n",
    "                         log=log_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f738d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSFT    0.070205\n",
       "BBBY    0.290349\n",
       "AAPL    0.198554\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diversify the portfolio weights using the DENSE correlation matrix.\n",
    "# This uses Pandas Series for the weights and DataFrame for the\n",
    "# corr. matrix, which are automatically converted to Numpy arrays.\n",
    "# The iterations of the algorithm are also recorded in the log.\n",
    "log_dense = []\n",
    "diversify_weights(weights_org=weights_org, corr=corr_dense, log=log_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "055ba858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Full Exp. 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Full Exp. 2</th>\n",
       "      <th>Weight 3</th>\n",
       "      <th>Full Exp. 3</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.132288</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.313209</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.201494</td>\n",
       "      <td>4.064010e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075593</td>\n",
       "      <td>0.105568</td>\n",
       "      <td>0.287348</td>\n",
       "      <td>0.297606</td>\n",
       "      <td>0.198517</td>\n",
       "      <td>0.199948</td>\n",
       "      <td>1.224562e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071606</td>\n",
       "      <td>0.101551</td>\n",
       "      <td>0.289659</td>\n",
       "      <td>0.299437</td>\n",
       "      <td>0.198568</td>\n",
       "      <td>0.200011</td>\n",
       "      <td>9.078328e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070512</td>\n",
       "      <td>0.100437</td>\n",
       "      <td>0.290203</td>\n",
       "      <td>0.299850</td>\n",
       "      <td>0.198557</td>\n",
       "      <td>0.200003</td>\n",
       "      <td>7.121535e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070205</td>\n",
       "      <td>0.100124</td>\n",
       "      <td>0.290349</td>\n",
       "      <td>0.299958</td>\n",
       "      <td>0.198554</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>5.670957e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weight 1  Full Exp. 1  Weight 2  Full Exp. 2  Weight 3  \\\n",
       "Iteration                                                           \n",
       "0          0.100000     0.132288  0.300000     0.313209  0.200000   \n",
       "1          0.075593     0.105568  0.287348     0.297606  0.198517   \n",
       "2          0.071606     0.101551  0.289659     0.299437  0.198568   \n",
       "3          0.070512     0.100437  0.290203     0.299850  0.198557   \n",
       "4          0.070205     0.100124  0.290349     0.299958  0.198554   \n",
       "\n",
       "           Full Exp. 3           MSE  \n",
       "Iteration                             \n",
       "0             0.201494  4.064010e-04  \n",
       "1             0.199948  1.224562e-05  \n",
       "2             0.200011  9.078328e-07  \n",
       "3             0.200003  7.121535e-08  \n",
       "4             0.200001  5.670957e-09  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log for the algorithm using the SPARSE corr. matrix.\n",
    "df_log_sparse = log_to_dataframe_sparse(log=log_sparse,\n",
    "                    weights_org=weights_org_np, corr_i=corr_i_np,\n",
    "                    corr_j=corr_j_np, corr_coef=corr_coef_np)\n",
    "df_log_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28126af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Full Exp. 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Full Exp. 2</th>\n",
       "      <th>Weight 3</th>\n",
       "      <th>Full Exp. 3</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.132288</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.313209</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.201494</td>\n",
       "      <td>4.064010e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075593</td>\n",
       "      <td>0.105568</td>\n",
       "      <td>0.287348</td>\n",
       "      <td>0.297606</td>\n",
       "      <td>0.198517</td>\n",
       "      <td>0.199948</td>\n",
       "      <td>1.224562e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071606</td>\n",
       "      <td>0.101551</td>\n",
       "      <td>0.289659</td>\n",
       "      <td>0.299437</td>\n",
       "      <td>0.198568</td>\n",
       "      <td>0.200011</td>\n",
       "      <td>9.078328e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070512</td>\n",
       "      <td>0.100437</td>\n",
       "      <td>0.290203</td>\n",
       "      <td>0.299850</td>\n",
       "      <td>0.198557</td>\n",
       "      <td>0.200003</td>\n",
       "      <td>7.121535e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070205</td>\n",
       "      <td>0.100124</td>\n",
       "      <td>0.290349</td>\n",
       "      <td>0.299958</td>\n",
       "      <td>0.198554</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>5.670957e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weight 1  Full Exp. 1  Weight 2  Full Exp. 2  Weight 3  \\\n",
       "Iteration                                                           \n",
       "0          0.100000     0.132288  0.300000     0.313209  0.200000   \n",
       "1          0.075593     0.105568  0.287348     0.297606  0.198517   \n",
       "2          0.071606     0.101551  0.289659     0.299437  0.198568   \n",
       "3          0.070512     0.100437  0.290203     0.299850  0.198557   \n",
       "4          0.070205     0.100124  0.290349     0.299958  0.198554   \n",
       "\n",
       "           Full Exp. 3           MSE  \n",
       "Iteration                             \n",
       "0             0.201494  4.064010e-04  \n",
       "1             0.199948  1.224562e-05  \n",
       "2             0.200011  9.078328e-07  \n",
       "3             0.200003  7.121535e-08  \n",
       "4             0.200001  5.670957e-09  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log for the algorithm using the DENSE corr. matrix.\n",
    "df_log_dense = log_to_dataframe(log=log_dense, weights_org=weights_org_np,\n",
    "                                corr=corr_dense)\n",
    "df_log_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d97c3734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           min  max\n",
       "Iteration          \n",
       "0          0.0  0.0\n",
       "1          0.0  0.0\n",
       "2          0.0  0.0\n",
       "3          0.0  0.0\n",
       "4          0.0  0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log's min/max difference between SPARSE and DENSE results.\n",
    "# These are all zero, which means the results of the SPARSE and DENSE\n",
    "# algorithms are exactly the same.\n",
    "df_log_dif = df_log_sparse - df_log_dense\n",
    "df_log_dif.T.describe().T[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e76b9e",
   "metadata": {},
   "source": [
    "### Sparse Correlation - Big Example\n",
    "\n",
    "Let us now consider a larger example with 1000 assets, where only about 1% of the correlation coefficients are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c20c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of assets in the portfolio.\n",
    "num_assets = 1000\n",
    "\n",
    "# Generate random portfolio weights.\n",
    "weights_org = rand_normal(rng=rng, size=num_assets,\n",
    "                          low=-1.0, high=1.0)\n",
    "\n",
    "# Generate random correlation matrix.\n",
    "corr_dense = rand_corr_normal(rng=rng, num_assets=num_assets)\n",
    "\n",
    "# Randomly set about 99% of the corr. matrix to zero.\n",
    "corr_dense = rand_zero(rng=rng, x=corr_dense, prob=0.99)\n",
    "\n",
    "# Make the correlation matrix symmetrical again.\n",
    "corr_dense += corr_dense.T\n",
    "\n",
    "# Ensure it is a valid correlation matrix.\n",
    "# This also clips the correlation coefficients between -1 and 1.\n",
    "fix_corr_matrix(corr=corr_dense, copy=False)\n",
    "\n",
    "# Only use the upper-triangle of the correlation matrix,\n",
    "# otherwise the sparse algorithm would use the correlations twice.\n",
    "corr_dense_triu = np.triu(corr_dense, k=1)\n",
    "\n",
    "# Convert to a sparse matrix in Coordinate (COO) format.\n",
    "corr_i, corr_j, corr_coef = matrix_to_sparse(mat=corr_dense_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d292867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix to a dense matrix.\n",
    "mat = sparse_to_matrix(shape=(num_assets, num_assets),\n",
    "           sparse_i=corr_i, sparse_j=corr_j, sparse_v=corr_coef)\n",
    "\n",
    "# Raise an exception if there are big differences.\n",
    "np.testing.assert_allclose(mat, corr_dense_triu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae5ab2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversify the portfolio weights using the SPARSE correlation matrix.\n",
    "# This uses Pandas Series for the weights and lists of strings for\n",
    "# corr_i and j, which are automatically converted to Numpy arrays.\n",
    "log_sparse = []\n",
    "weights_new_sparse = diversify_weights_sparse(weights_org=weights_org,\n",
    "                                corr_i=corr_i, corr_j=corr_j,\n",
    "                                corr_coef=corr_coef, log=log_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f51bd6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversify the portfolio weights using the DENSE correlation matrix.\n",
    "# This uses Pandas Series for the weights and DataFrame for the corr.\n",
    "# matrix, which are automatically converted to Numpy arrays.\n",
    "log_dense = []\n",
    "weights_new_dense = diversify_weights(weights_org=weights_org,\n",
    "                                corr=corr_dense, log=log_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3314fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the diversified weights are nearly identical\n",
    "# when calculated by the SPARSE and DENSE algorithms.\n",
    "# This raises an exception if there are big differences.\n",
    "np.testing.assert_allclose(weights_new_sparse, weights_new_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23801ad1",
   "metadata": {},
   "source": [
    "### Sparse vs. Dense Algorithms Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7078b37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Full Exp. 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Full Exp. 2</th>\n",
       "      <th>Weight 3</th>\n",
       "      <th>Full Exp. 3</th>\n",
       "      <th>Weight 4</th>\n",
       "      <th>Full Exp. 4</th>\n",
       "      <th>Weight 5</th>\n",
       "      <th>Full Exp. 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Full Exp. 996</th>\n",
       "      <th>Weight 997</th>\n",
       "      <th>Full Exp. 997</th>\n",
       "      <th>Weight 998</th>\n",
       "      <th>Full Exp. 998</th>\n",
       "      <th>Weight 999</th>\n",
       "      <th>Full Exp. 999</th>\n",
       "      <th>Weight 1000</th>\n",
       "      <th>Full Exp. 1000</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045931</td>\n",
       "      <td>-0.052103</td>\n",
       "      <td>-0.040210</td>\n",
       "      <td>-0.045229</td>\n",
       "      <td>-0.046094</td>\n",
       "      <td>-0.046625</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>0.082357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018791</td>\n",
       "      <td>-0.025225</td>\n",
       "      <td>-0.033810</td>\n",
       "      <td>4.264695e-05</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>-0.006187</td>\n",
       "      <td>-0.012436</td>\n",
       "      <td>-0.015360</td>\n",
       "      <td>3.613347e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040491</td>\n",
       "      <td>-0.045555</td>\n",
       "      <td>-0.035748</td>\n",
       "      <td>-0.039917</td>\n",
       "      <td>-0.045569</td>\n",
       "      <td>-0.045964</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.038425</td>\n",
       "      <td>0.065968</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013370</td>\n",
       "      <td>-0.018820</td>\n",
       "      <td>-0.026022</td>\n",
       "      <td>2.941466e-06</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.001738</td>\n",
       "      <td>-0.004039</td>\n",
       "      <td>-0.010068</td>\n",
       "      <td>-0.012520</td>\n",
       "      <td>6.825531e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040826</td>\n",
       "      <td>-0.045796</td>\n",
       "      <td>-0.036009</td>\n",
       "      <td>-0.040125</td>\n",
       "      <td>-0.045698</td>\n",
       "      <td>-0.046080</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.066319</td>\n",
       "      <td>0.073679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012549</td>\n",
       "      <td>-0.018243</td>\n",
       "      <td>-0.025410</td>\n",
       "      <td>8.369937e-07</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.012451</td>\n",
       "      <td>8.977922e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040947</td>\n",
       "      <td>-0.045883</td>\n",
       "      <td>-0.036086</td>\n",
       "      <td>-0.040185</td>\n",
       "      <td>-0.045712</td>\n",
       "      <td>-0.046091</td>\n",
       "      <td>0.032938</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>0.066345</td>\n",
       "      <td>0.073715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012291</td>\n",
       "      <td>-0.018110</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>4.484521e-07</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>-0.003410</td>\n",
       "      <td>-0.009988</td>\n",
       "      <td>-0.012440</td>\n",
       "      <td>1.516588e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040990</td>\n",
       "      <td>-0.045913</td>\n",
       "      <td>-0.036108</td>\n",
       "      <td>-0.040202</td>\n",
       "      <td>-0.045715</td>\n",
       "      <td>-0.046094</td>\n",
       "      <td>0.032938</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>0.066339</td>\n",
       "      <td>0.073714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012204</td>\n",
       "      <td>-0.018077</td>\n",
       "      <td>-0.025237</td>\n",
       "      <td>3.287387e-07</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>-0.003336</td>\n",
       "      <td>-0.009984</td>\n",
       "      <td>-0.012437</td>\n",
       "      <td>2.871265e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weight 1  Full Exp. 1  Weight 2  Full Exp. 2  Weight 3  \\\n",
       "Iteration                                                           \n",
       "0         -0.045931    -0.052103 -0.040210    -0.045229 -0.046094   \n",
       "1         -0.040491    -0.045555 -0.035748    -0.039917 -0.045569   \n",
       "2         -0.040826    -0.045796 -0.036009    -0.040125 -0.045698   \n",
       "3         -0.040947    -0.045883 -0.036086    -0.040185 -0.045712   \n",
       "4         -0.040990    -0.045913 -0.036108    -0.040202 -0.045715   \n",
       "\n",
       "           Full Exp. 3  Weight 4  Full Exp. 4  Weight 5  Full Exp. 5  ...  \\\n",
       "Iteration                                                             ...   \n",
       "0            -0.046625  0.038366     0.044617  0.073708     0.082357  ...   \n",
       "1            -0.045964  0.032992     0.038425  0.065968     0.073318  ...   \n",
       "2            -0.046080  0.032941     0.038370  0.066319     0.073679  ...   \n",
       "3            -0.046091  0.032938     0.038367  0.066345     0.073715  ...   \n",
       "4            -0.046094  0.032938     0.038367  0.066339     0.073714  ...   \n",
       "\n",
       "           Full Exp. 996  Weight 997  Full Exp. 997    Weight 998  \\\n",
       "Iteration                                                           \n",
       "0              -0.018791   -0.025225      -0.033810  4.264695e-05   \n",
       "1              -0.013370   -0.018820      -0.026022  2.941466e-06   \n",
       "2              -0.012549   -0.018243      -0.025410  8.369937e-07   \n",
       "3              -0.012291   -0.018110      -0.025271  4.484521e-07   \n",
       "4              -0.012204   -0.018077      -0.025237  3.287387e-07   \n",
       "\n",
       "           Full Exp. 998  Weight 999  Full Exp. 999  Weight 1000  \\\n",
       "Iteration                                                          \n",
       "0               0.000618   -0.003279      -0.006187    -0.012436   \n",
       "1               0.000150   -0.001738      -0.004039    -0.010068   \n",
       "2               0.000080   -0.001411      -0.003588    -0.010000   \n",
       "3               0.000058   -0.001290      -0.003410    -0.009988   \n",
       "4               0.000050   -0.001240      -0.003336    -0.009984   \n",
       "\n",
       "           Full Exp. 1000           MSE  \n",
       "Iteration                                \n",
       "0               -0.015360  3.613347e-05  \n",
       "1               -0.012520  6.825531e-07  \n",
       "2               -0.012451  8.977922e-08  \n",
       "3               -0.012440  1.516588e-08  \n",
       "4               -0.012437  2.871265e-09  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log for the SPARSE algorithm.\n",
    "df_log_sparse = log_to_dataframe_sparse(weights_org=weights_org,\n",
    "                    corr_i=corr_i, corr_j=corr_j, corr_coef=corr_coef, log=log_sparse)\n",
    "df_log_sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8eaab281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight 1</th>\n",
       "      <th>Full Exp. 1</th>\n",
       "      <th>Weight 2</th>\n",
       "      <th>Full Exp. 2</th>\n",
       "      <th>Weight 3</th>\n",
       "      <th>Full Exp. 3</th>\n",
       "      <th>Weight 4</th>\n",
       "      <th>Full Exp. 4</th>\n",
       "      <th>Weight 5</th>\n",
       "      <th>Full Exp. 5</th>\n",
       "      <th>...</th>\n",
       "      <th>Full Exp. 996</th>\n",
       "      <th>Weight 997</th>\n",
       "      <th>Full Exp. 997</th>\n",
       "      <th>Weight 998</th>\n",
       "      <th>Full Exp. 998</th>\n",
       "      <th>Weight 999</th>\n",
       "      <th>Full Exp. 999</th>\n",
       "      <th>Weight 1000</th>\n",
       "      <th>Full Exp. 1000</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045931</td>\n",
       "      <td>-0.052103</td>\n",
       "      <td>-0.040210</td>\n",
       "      <td>-0.045229</td>\n",
       "      <td>-0.046094</td>\n",
       "      <td>-0.046625</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>0.044617</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>0.082357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018791</td>\n",
       "      <td>-0.025225</td>\n",
       "      <td>-0.033810</td>\n",
       "      <td>4.264695e-05</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>-0.006187</td>\n",
       "      <td>-0.012436</td>\n",
       "      <td>-0.015360</td>\n",
       "      <td>3.613347e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040491</td>\n",
       "      <td>-0.045555</td>\n",
       "      <td>-0.035748</td>\n",
       "      <td>-0.039917</td>\n",
       "      <td>-0.045569</td>\n",
       "      <td>-0.045964</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.038425</td>\n",
       "      <td>0.065968</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013370</td>\n",
       "      <td>-0.018820</td>\n",
       "      <td>-0.026022</td>\n",
       "      <td>2.941466e-06</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.001738</td>\n",
       "      <td>-0.004039</td>\n",
       "      <td>-0.010068</td>\n",
       "      <td>-0.012520</td>\n",
       "      <td>6.825531e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040826</td>\n",
       "      <td>-0.045796</td>\n",
       "      <td>-0.036009</td>\n",
       "      <td>-0.040125</td>\n",
       "      <td>-0.045698</td>\n",
       "      <td>-0.046080</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>0.066319</td>\n",
       "      <td>0.073679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012549</td>\n",
       "      <td>-0.018243</td>\n",
       "      <td>-0.025410</td>\n",
       "      <td>8.369937e-07</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>-0.001411</td>\n",
       "      <td>-0.003588</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-0.012451</td>\n",
       "      <td>8.977922e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040947</td>\n",
       "      <td>-0.045883</td>\n",
       "      <td>-0.036086</td>\n",
       "      <td>-0.040185</td>\n",
       "      <td>-0.045712</td>\n",
       "      <td>-0.046091</td>\n",
       "      <td>0.032938</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>0.066345</td>\n",
       "      <td>0.073715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012291</td>\n",
       "      <td>-0.018110</td>\n",
       "      <td>-0.025271</td>\n",
       "      <td>4.484521e-07</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>-0.003410</td>\n",
       "      <td>-0.009988</td>\n",
       "      <td>-0.012440</td>\n",
       "      <td>1.516588e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040990</td>\n",
       "      <td>-0.045913</td>\n",
       "      <td>-0.036108</td>\n",
       "      <td>-0.040202</td>\n",
       "      <td>-0.045715</td>\n",
       "      <td>-0.046094</td>\n",
       "      <td>0.032938</td>\n",
       "      <td>0.038367</td>\n",
       "      <td>0.066339</td>\n",
       "      <td>0.073714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012204</td>\n",
       "      <td>-0.018077</td>\n",
       "      <td>-0.025237</td>\n",
       "      <td>3.287387e-07</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.001240</td>\n",
       "      <td>-0.003336</td>\n",
       "      <td>-0.009984</td>\n",
       "      <td>-0.012437</td>\n",
       "      <td>2.871265e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Weight 1  Full Exp. 1  Weight 2  Full Exp. 2  Weight 3  \\\n",
       "Iteration                                                           \n",
       "0         -0.045931    -0.052103 -0.040210    -0.045229 -0.046094   \n",
       "1         -0.040491    -0.045555 -0.035748    -0.039917 -0.045569   \n",
       "2         -0.040826    -0.045796 -0.036009    -0.040125 -0.045698   \n",
       "3         -0.040947    -0.045883 -0.036086    -0.040185 -0.045712   \n",
       "4         -0.040990    -0.045913 -0.036108    -0.040202 -0.045715   \n",
       "\n",
       "           Full Exp. 3  Weight 4  Full Exp. 4  Weight 5  Full Exp. 5  ...  \\\n",
       "Iteration                                                             ...   \n",
       "0            -0.046625  0.038366     0.044617  0.073708     0.082357  ...   \n",
       "1            -0.045964  0.032992     0.038425  0.065968     0.073318  ...   \n",
       "2            -0.046080  0.032941     0.038370  0.066319     0.073679  ...   \n",
       "3            -0.046091  0.032938     0.038367  0.066345     0.073715  ...   \n",
       "4            -0.046094  0.032938     0.038367  0.066339     0.073714  ...   \n",
       "\n",
       "           Full Exp. 996  Weight 997  Full Exp. 997    Weight 998  \\\n",
       "Iteration                                                           \n",
       "0              -0.018791   -0.025225      -0.033810  4.264695e-05   \n",
       "1              -0.013370   -0.018820      -0.026022  2.941466e-06   \n",
       "2              -0.012549   -0.018243      -0.025410  8.369937e-07   \n",
       "3              -0.012291   -0.018110      -0.025271  4.484521e-07   \n",
       "4              -0.012204   -0.018077      -0.025237  3.287387e-07   \n",
       "\n",
       "           Full Exp. 998  Weight 999  Full Exp. 999  Weight 1000  \\\n",
       "Iteration                                                          \n",
       "0               0.000618   -0.003279      -0.006187    -0.012436   \n",
       "1               0.000150   -0.001738      -0.004039    -0.010068   \n",
       "2               0.000080   -0.001411      -0.003588    -0.010000   \n",
       "3               0.000058   -0.001290      -0.003410    -0.009988   \n",
       "4               0.000050   -0.001240      -0.003336    -0.009984   \n",
       "\n",
       "           Full Exp. 1000           MSE  \n",
       "Iteration                                \n",
       "0               -0.015360  3.613347e-05  \n",
       "1               -0.012520  6.825531e-07  \n",
       "2               -0.012451  8.977922e-08  \n",
       "3               -0.012440  1.516588e-08  \n",
       "4               -0.012437  2.871265e-09  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log for the DENSE algorithm.\n",
    "df_log_dense = log_to_dataframe(weights_org=weights_org,\n",
    "                                corr=corr_dense, log=log_dense)\n",
    "df_log_dense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "872374be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iteration</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.387779e-17</td>\n",
       "      <td>1.387779e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>2.775558e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8.326673e-17</td>\n",
       "      <td>2.775558e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>2.775558e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.775558e-17</td>\n",
       "      <td>2.775558e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    min           max\n",
       "Iteration                            \n",
       "0         -1.387779e-17  1.387779e-17\n",
       "1         -2.775558e-17  2.775558e-17\n",
       "2         -8.326673e-17  2.775558e-17\n",
       "3         -2.775558e-17  2.775558e-17\n",
       "4         -2.775558e-17  2.775558e-17"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the log's min/max difference between SPARSE and DENSE results.\n",
    "df_log_dif = df_log_sparse - df_log_dense\n",
    "df_log_dif.T.describe().T[['min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ae29e",
   "metadata": {},
   "source": [
    "Note how the results are slightly different between the sparse and dense diversification algorithms. Mathematically speaking they are calculating the exact same thing, but because the computations are ordered differently in the two algorithms, the floating-point computations are rounded slightly differently, thus giving small differences in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123bea95",
   "metadata": {},
   "source": [
    "### Sparse vs. Dense Algorithms Time-Usage\n",
    "\n",
    "The following shows that the SPARSE algorithms are roughly 25-30 times faster than the DENSE algorithms, even though the SPARSE correlation matrix only has about 1% of the correlation coefficients of the DENSE matrix, so we might expect the SPARSE algorithms to be 100 times faster than the DENSE algorithms. The difference is probably due to various kinds of computational overhead, so the time-usage of the SPARSE algorithm does not scale perfectly with the sparsity of the correlation matrix.\n",
    "\n",
    "The [short paper](#refs) referenced above has more detailed comparisons of the time-usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84c87fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 s  5.6 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n",
      "11.1 ms  155 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Compare time-usage of the SPARSE and DENSE diversification algorithms.\n",
    "%timeit diversify_weights_sparse(weights_org=weights_org, corr_i=corr_i, corr_j=corr_j, corr_coef=corr_coef)\n",
    "%timeit diversify_weights(weights_org=weights_org, corr=corr_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1bc9a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.2 s  934 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)\n",
      "1.96 ms  17 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Compare time-usage of the SPARSE and DENSE Full Exposure algorithms.\n",
    "%timeit full_exposure_sparse(weights=weights_org, corr_i=corr_i, corr_j=corr_j, corr_coef=corr_coef)\n",
    "%timeit full_exposure(weights=weights_org, corr=corr_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd288f03",
   "metadata": {},
   "source": [
    "## Parallel vs. Serial Execution\n",
    "\n",
    "The function `full_exposure_par` is the parallel version of the function `full_exposure`, and the function `diversify_weights` can be run in parallel mode by passing the argument `parallel=True`.\n",
    "\n",
    "For portfolios with less than 200 assets, the serial version is probably faster than the parallel version. You should conduct timing experiments like the following for your particular use-case and computer, to see whether the parallel or serial version is faster for you.\n",
    "\n",
    "Note that the functions for using sparse correlation matrices currently do not support parallel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "293df5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the parallel versions once so their Numba JIT compilations\n",
    "# do not interfere with the timing measurements below.\n",
    "full_exposure_par(weights=weights_org, corr=corr_dense);\n",
    "diversify_weights(weights_org=weights_org, corr=corr_dense, parallel=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ab0c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2 ms  4.4 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n",
      "5.37 ms  4.89 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Compare time-usage of the SERIAL and PARALLEL diversification algorithms.\n",
    "# Serial version.\n",
    "%timeit diversify_weights(weights_org=weights_org, corr=corr_dense)\n",
    "# Parallel version.\n",
    "# Note the time-usage may not scale perfectly with the number of\n",
    "# CPU cores, because only a part of the algorithm is run in parallel.\n",
    "%timeit diversify_weights(weights_org=weights_org, corr=corr_dense, parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80c7e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ms  58 s per loop (mean  std. dev. of 7 runs, 100 loops each)\n",
      "322 s  660 ns per loop (mean  std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Compare time-usage of the SERIAL and PARALLEL Full Exposure algorithms.\n",
    "# Serial version.\n",
    "%timeit full_exposure(weights=weights_org, corr=corr_dense)\n",
    "# Parallel version.\n",
    "%timeit full_exposure_par(weights=weights_org, corr=corr_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20c4ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the number of execution threads as follows.\n",
    "\n",
    "# Save the original number of execution threads that are available.\n",
    "nb_threads = nb.config.NUMBA_NUM_THREADS\n",
    "\n",
    "# Set the number of execution threads to 1, which effectively\n",
    "# makes the code run in serial mode. On Linux there is no\n",
    "# performance penalty versus using parallel mode, see the following:\n",
    "# https://numba.discourse.group/t/switching-between-parallel-serial-mode/1125/3\n",
    "nb.set_num_threads(1)\n",
    "\n",
    "# Set the number of execution threads to 2.\n",
    "nb.set_num_threads(2)\n",
    "\n",
    "# Restore the original number of execution threads available.\n",
    "nb.set_num_threads(nb_threads)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de6f7f",
   "metadata": {},
   "source": [
    "## License (MIT)\n",
    "\n",
    "This is published under the [MIT License](https://github.com/Hvass-Labs/InvestOps-Tutorials/blob/main/LICENSE) which allows very broad use for both academic and commercial purposes.\n",
    "\n",
    "You are very welcome to modify and use this source-code in your own project. Please keep a link to the [original repository](https://github.com/Hvass-Labs/InvestOps-Tutorials)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
